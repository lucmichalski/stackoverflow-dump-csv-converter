#!/usr/local/anaconda3/bin/python3

import xml.etree.ElementTree as et
import csv
import codecs
from tqdm import tqdm
import re
import subprocess
import sys
import json
import os.path
from glob import glob

def main(dataset, workdir, release, limit = -1):
    """Convert a Stackoverflow dataset from XML to CSV

    Generic function that converts Stackoverflow XML datasets to CSV.

    Args:
        dataset (str): Dataset name, i.e. "Tags"
        limit   (int): Number of lines to process
    """
    print("Initiating process.")
    output = None

    #Split files containing more lines than
    large_file_threshold = 55000000

    #Converted file already exists, exit process.
    if os.path.exists(workdir + "/" + dataset + ".csv"):
        print("Found converted file " + workdir + "/" + dataset + ".csv. Exiting.")
        sys.exit(1)

    #Load dataset configuration
    global config
    if dataset in config:
        config = config[dataset]
        print("Config OK.")
    else:
        print("ERROR: Cannot find config item for " + dataset + ".")
        sys.exit(1)

    header_line = config["header_line"]

    fixes = {}
    if "fixes" in config:
        fixes = config["fixes"]

    #Load meta
    with open("meta.json", "r") as f:
        meta = json.load(f)

    line_numbers = meta[release]["line_numbers"][dataset]
    print("Meta loaded.")

    #Prepare files
    if line_numbers > large_file_threshold:
        #Split and convert files if no partial CSV exists
        if not os.path.exists(workdir + "/" + dataset + "_00.csv"):
            #Split files that contain more lines than the threshold
            print("File is large, splitting...")
            file = workdir + "/" + dataset + ".xml"

            if os.path.exists(workdir + "/" + dataset + "_00"):
                #If split file exists do not split again, skip split
                print("Found split file, skipping split.")
            else:
                #Split file does not exists, do splist
                subprocess.run(["split", "-l " + str(large_file_threshold), "-d", file, workdir + "/" + dataset + "_"])

            #Process split files
            print("Split done, adding head and tail.")
            head = subprocess.check_output(["head", "-n2", file]).decode("utf-8")
            tail = subprocess.check_output(["tail", "-n1", file]).decode("utf-8")

            partials = sorted(glob(workdir + "/" + dataset + "_*"))

            #Add head and tail to split files, to get well formed XML
            for f in partials:
                if not re.search(r'xml\sversion', subprocess.check_output(["head", "-n1", f]).decode("utf-8")):
                    subprocess.run('echo -n "' + head.replace('"', '\\"') + '" | cat - ' + os.path.abspath(f) + ' > /tmp/out && mv /tmp/out ' + os.path.abspath(f), shell=True)
                if not re.search(r'^<\/', subprocess.check_output(["tail", "-n1", f]).decode("utf-8")):
                    subprocess.run('echo "' + tail + '" >> ' + os.path.abspath(f), shell=True)

            #Convert split files
            for f in partials:
                suffix = re.search(r'\d+', os.path.basename(f)).group()
                output_file = workdir + "/" + dataset + "_" + suffix + ".csv"

                xml_iter = et.iterparse(f, events=('start', 'end'))

                try:
                    output = codecs.open(output_file, "w", "utf-8")
                except:
                    print("Failed to open the output file")
                    raise

                print("Invoking conversion of " + os.path.basename(f))

                ln = large_file_threshold +1 if len(partials) > int(suffix) -1 else line_numbers - int(suffix) * large_file_threshold
                #Convert
                convert(output, xml_iter, ln, header_line, fixes, limit)

                print("Conversion done, file written " + output_file)
        else:
            print("Partial CSV files found skipping conversion.")

        #Combine partial CSVs
#         print("Combining partial CSV files.")
        # csvs = sorted(glob(workdir + "/" + dataset + "_*.csv"))
        # for f in csvs:
#             subprocess.run('cat ' + os.path.abspath(f) + ' >> ' + workdir + '/' + dataset + '.csv >> /dev/null', shell=True)
    else:
        #File is not large, do simple conversion
        input_file = workdir + "/" + dataset + ".xml"
        output_file = workdir + "/" + dataset + ".csv"

        xml_iter = et.iterparse(input_file, events=('start', 'end'))

        try:
            output = codecs.open(output_file, "w", "utf-8")
        except:
            print("Failed to open the output file")
            raise

        print("Starting conversion.")

        #Convert
        convert(output, xml_iter, line_numbers-1, header_line, fixes, limit)

    print("All DONE.")
    sys.exit(0)

def convert(output, xml_iter, line_numbers, header_line, fixes, limit):
    output_buffer = []
    buffer_size = 1000
    delimiter=","
    tag = "row"
    n = 0

    output.write(delimiter.join(header_line) + '\n')
    print("File initialized, starting conversion...")

    with tqdm(total=line_numbers, unit="lines") as pbar:
        for event, elem in xml_iter:
            if event == "end":
                if elem.tag == tag:
                    # if "docker" in elem.get("Tags", "") or "kubernetes" in elem.get("Tags", ""):
                    #collect items from element
                    items = []
                    for h in header_line:
                        d = elem.get(h, "")
                        if h in fixes:
                           d = re.sub(fixes[h][0], fixes[h][1],d)
                        items.append(d)
                        # items.append(elem.get(h, ""))
                    #append items to output buffer
                    output_buffer.append(r'"' + (r'"' + delimiter + r'"').join(items) + r'"')

                    #manage limit
                    n += 1
                    if n == limit:
                        break

                    #write to file if buffer full
                    if len(output_buffer) > buffer_size:
                        output.write('\n'.join(output_buffer) + '\n')
                        output_buffer = []

                elem.clear()
                pbar.update(1)

    output.write('\n'.join(output_buffer) + '\n')
    output.close()

if len(sys.argv) == 1:
    print("""Usage:
    python3 contverter.py DATASET WORKDIR RELEASE

    Arguments:
    - DATASET    name of the Stack Exchange dataset (e.g. Posts)
    - WORKDIR    directory where the dataset is located
    - RELEASE    release version of the dataset in YYYY_MM format (e.g. 2018_12)""")
else:
    main(*sys.argv[1:])
